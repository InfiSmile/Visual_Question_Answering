{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3798293,"sourceType":"datasetVersion","datasetId":2264789}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport wandb\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport os\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom torch import nn\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom torchvision import transforms, models\nfrom transformers import BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:43:59.673708Z","iopub.execute_input":"2024-12-06T15:43:59.674544Z","iopub.status.idle":"2024-12-06T15:44:00.365230Z","shell.execute_reply.started":"2024-12-06T15:43:59.674518Z","shell.execute_reply":"2024-12-06T15:44:00.364555Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"pip install jupyterlab_widgets==2.0.0a0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T07:00:38.832088Z","iopub.execute_input":"2024-12-06T07:00:38.832893Z","iopub.status.idle":"2024-12-06T07:00:49.182030Z","shell.execute_reply.started":"2024-12-06T07:00:38.832852Z","shell.execute_reply":"2024-12-06T07:00:49.180987Z"}},"outputs":[{"name":"stdout","text":"Collecting jupyterlab_widgets==2.0.0a0\n  Downloading jupyterlab_widgets-2.0.0a0-py3-none-any.whl.metadata (3.5 kB)\nDownloading jupyterlab_widgets-2.0.0a0-py3-none-any.whl (259 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.2/259.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: jupyterlab_widgets\n  Attempting uninstall: jupyterlab_widgets\n    Found existing installation: jupyterlab-widgets 3.0.9\n    Uninstalling jupyterlab-widgets-3.0.9:\n      Successfully uninstalled jupyterlab-widgets-3.0.9\nSuccessfully installed jupyterlab_widgets-2.0.0a0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:43:15.829798Z","iopub.execute_input":"2024-12-06T15:43:15.830116Z","iopub.status.idle":"2024-12-06T15:43:15.859725Z","shell.execute_reply.started":"2024-12-06T15:43:15.830093Z","shell.execute_reply":"2024-12-06T15:43:15.858797Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:18:26.808356Z","iopub.execute_input":"2024-12-06T15:18:26.809242Z","iopub.status.idle":"2024-12-06T15:18:26.816287Z","shell.execute_reply.started":"2024-12-06T15:18:26.809214Z","shell.execute_reply":"2024-12-06T15:18:26.815594Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\napi_key = \"wandb-api-key\"\nwandb.login(key=UserSecretsClient().get_secret(api_key), relogin=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-05T23:59:19.392546Z","iopub.execute_input":"2024-12-05T23:59:19.393009Z","iopub.status.idle":"2024-12-05T23:59:19.916729Z","shell.execute_reply.started":"2024-12-05T23:59:19.392977Z","shell.execute_reply":"2024-12-05T23:59:19.915349Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      2\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb-api-key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[43mUserSecretsClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_secret\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m, relogin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_secrets.py:64\u001b[0m, in \u001b[0;36mUserSecretsClient.get_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel must be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m request_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label,\n\u001b[1;32m     63\u001b[0m }\n\u001b[0;32m---> 64\u001b[0m response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweb_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_post_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecret\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendError(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected response from the service. Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/kaggle_web_client.py:49\u001b[0m, in \u001b[0;36mKaggleWebClient.make_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m         response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwasSuccessful\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[0;32m---> 49\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m BackendError(\n\u001b[1;32m     50\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected response from the service. Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (URLError, socket\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 72318224 and label wandb-api-key.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}."],"ename":"BackendError","evalue":"Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 72318224 and label wandb-api-key.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}.","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"with open(os.path.join(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\", \"answer_space.txt\")) as f:\n            # print(f.read())\n            answer_space = f.read().splitlines()\n            \nprint(len(answer_space))      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T13:00:00.302655Z","iopub.execute_input":"2024-12-06T13:00:00.303075Z","iopub.status.idle":"2024-12-06T13:00:00.311246Z","shell.execute_reply.started":"2024-12-06T13:00:00.303044Z","shell.execute_reply":"2024-12-06T13:00:00.309786Z"}},"outputs":[{"name":"stdout","text":"582\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, csv_file, image_folder, transform=None):\n        # Load dataset from CSV\n        self.data = pd.read_csv(csv_file)\n        self.image_folder = image_folder\n        self.transform = transform\n        # Initialize the tokenizer\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        # Preprocess the dataset\n        self.preprocess_dataset()\n\n    def preprocess_dataset(self):\n        dataset = load_dataset(\n            \"csv\",\n            data_files={\n                \"train\": os.path.join(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\", \"data_train.csv\"),\n                \"test\": os.path.join(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\", \"data_eval.csv\")\n            }\n        )\n        # print(dataset)\n\n        with open(os.path.join(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\", \"answer_space.txt\")) as f:\n            answer_space = f.read().splitlines()\n\n        self.data = dataset.map(\n            lambda examples: {\n                'label': [\n                    answer_space.index(ans.replace(\" \", \"\").split(\",\")[0])  \n                    for ans in examples['answer']\n                ]\n            },\n            batched=True\n        )\n\n        # Convert dataset to DataFrame for easy indexing\n        self.data = pd.DataFrame(self.data['train'])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        image_id = row['image_id']\n        question = row['question']\n        label = row['label']\n\n        image_path = os.path.join(self.image_folder, f\"{image_id}.png\")\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n        # Tokenize the question\n        inputs = self.tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n        # print(f'inputs: {inputs}')\n        \n        # Ensure the tensors are in the correct format for the DataLoader\n        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n        # print(input_ids.shape)\n        attention_mask = inputs['attention_mask'].squeeze(0)  # Remove batch dimension\n        # print(f'attention_mask: {attention_mask.shape}')\n\n        return image, input_ids, attention_mask, label","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:50:22.330371Z","iopub.execute_input":"2024-12-06T15:50:22.330613Z","iopub.status.idle":"2024-12-06T15:50:22.342185Z","shell.execute_reply.started":"2024-12-06T15:50:22.330592Z","shell.execute_reply":"2024-12-06T15:50:22.341224Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"image, input_ids, attention_mask, label = train_dataset[0]\n\n# Print the outputs\nprint(f\"Image: {image.shape}\")\nprint(f\"Input IDs: {input_ids.shape}\")\nprint(f\"Attention Mask: {attention_mask.shape}\")\nprint(f\"Label: {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T12:34:12.627650Z","iopub.execute_input":"2024-12-06T12:34:12.628128Z","iopub.status.idle":"2024-12-06T12:34:12.661487Z","shell.execute_reply.started":"2024-12-06T12:34:12.628097Z","shell.execute_reply":"2024-12-06T12:34:12.660153Z"}},"outputs":[{"name":"stdout","text":"inputs: {'input_ids': tensor([[  101,  2054,  2003,  1996,  4874,  2006,  1996, 15475,   102,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}\ntorch.Size([512])\nattention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\nImage: torch.Size([3, 224, 224])\nInput IDs: torch.Size([512])\nAttention Mask: torch.Size([512])\nLabel: 149\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\ntrain_dataset = VQADataset('/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/data_train.csv', '/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/images', transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\nval_dataset = VQADataset('/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/data_eval.csv', '/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/images', transform)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:50:22.346566Z","iopub.execute_input":"2024-12-06T15:50:22.346824Z","iopub.status.idle":"2024-12-06T15:50:24.851378Z","shell.execute_reply.started":"2024-12-06T15:50:22.346797Z","shell.execute_reply":"2024-12-06T15:50:24.850465Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\nfrom torchvision import models\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.checkpoint import checkpoint\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport json\n\nclass VQAModel(nn.Module):\n    def __init__(self, num_answers):\n        super(VQAModel, self).__init__()\n        # Image feature extractor\n        self.cnn = models.resnet50(pretrained=True)\n        self.cnn.fc = nn.Identity()  # Remove the final classification layer\n\n        # Question feature extractor\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Fusion and final classification\n        self.fc1 = nn.Linear(2048 + 768, 1024)\n        self.fc2 = nn.Linear(1024, num_answers)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, images, input_ids, attention_mask):\n        # Extract image features\n        image_features = checkpoint(self.cnn, images)  # Use checkpointing for ResNet\n        # print(f'image_features {image_features}')\n        # print(f'shape of image feature is {image_features.shape}') #([64, 2048])\n        # Extract question features\n        outputs = checkpoint(self.bert, input_ids, attention_mask)\n        question_features = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, 768) #CLS TOKEN\n        # print(f'question feature shape {question_features.shape}')\n        # Concatenate features\n        combined_features = torch.cat((image_features, question_features), dim=1)\n        # print(f'combined_features shape {combined_features.shape}') #([64, 2816])\n        # Classification\n        x = self.fc1(combined_features)\n        # print(f' 1 *************** {x.shape}') #([64, 1024])\n        x = self.dropout(x)\n        # print(f'dropout {x.shape} ')#([64, 1024])\n        x = self.fc2(x)\n        # print(f'final layer {x.shape}') #([64, 582])\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:56:33.020726Z","iopub.execute_input":"2024-12-06T15:56:33.021383Z","iopub.status.idle":"2024-12-06T15:56:33.033590Z","shell.execute_reply.started":"2024-12-06T15:56:33.021342Z","shell.execute_reply":"2024-12-06T15:56:33.032660Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.nn import CrossEntropyLoss\n\n# Initialize the model\nmodel = VQAModel(num_answers=582)  #print(len(answer_space)) length of answer space is 582\nmodel.to(device)\n\n# Training parameters\nnum_epochs = 100\nlr = 0.0005 \nweight_decay = 1e-4\nbest_loss = float('inf')\nbest_model_state = None\npatience = 10  # Number of epochs to wait for improvement before stopping\nearly_stopping_counter = 0\n\n# Initialize the optimizer and GradScaler for mixed precision training\noptimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\nscaler = GradScaler()\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define scheduler\nscheduler_step_size = int(num_epochs * 0.25)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=scheduler_step_size)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:56:36.001019Z","iopub.execute_input":"2024-12-06T15:56:36.001664Z","iopub.status.idle":"2024-12-06T15:56:37.351716Z","shell.execute_reply.started":"2024-12-06T15:56:36.001639Z","shell.execute_reply":"2024-12-06T15:56:37.351051Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"pip install --upgrade ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T07:01:55.275994Z","iopub.execute_input":"2024-12-06T07:01:55.276339Z","iopub.status.idle":"2024-12-06T07:02:04.724597Z","shell.execute_reply.started":"2024-12-06T07:01:55.276297Z","shell.execute_reply":"2024-12-06T07:02:04.723599Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nCollecting ipywidgets\n  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\nRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nCollecting widgetsnbextension~=4.0.12 (from ipywidgets)\n  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\nDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n  Attempting uninstall: widgetsnbextension\n    Found existing installation: widgetsnbextension 3.6.6\n    Uninstalling widgetsnbextension-3.6.6:\n      Successfully uninstalled widgetsnbextension-3.6.6\n  Attempting uninstall: jupyterlab-widgets\n    Found existing installation: jupyterlab-widgets 2.0.0a0\n    Uninstalling jupyterlab-widgets-2.0.0a0:\n      Successfully uninstalled jupyterlab-widgets-2.0.0a0\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.7.1\n    Uninstalling ipywidgets-7.7.1:\n      Successfully uninstalled ipywidgets-7.7.1\nSuccessfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# def calculate_accuracy(outputs, labels):\n#     _, preds = torch.max(outputs, 1)\n#     correct = (preds == labels).sum().item()\n#     total = labels.size(0)\n#     return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-07-04T01:42:41.984722Z","iopub.execute_input":"2024-07-04T01:42:41.985008Z","iopub.status.idle":"2024-07-04T01:42:41.990031Z","shell.execute_reply.started":"2024-07-04T01:42:41.984983Z","shell.execute_reply":"2024-07-04T01:42:41.989067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_accuracy(outputs, labels):\n        _, preds = torch.max(outputs, 1)\n        correct = (preds == labels).float().sum()\n        accuracy = correct / labels.size(0)\n        return accuracy.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:56:42.936619Z","iopub.execute_input":"2024-12-06T15:56:42.937421Z","iopub.status.idle":"2024-12-06T15:56:42.941642Z","shell.execute_reply.started":"2024-12-06T15:56:42.937392Z","shell.execute_reply":"2024-12-06T15:56:42.940811Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport wandb\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scaler, device, num_epochs, patience, save_path, project_name):\n    # Initialize wandb\n    wandb.init(project=project_name)\n    wandb.watch(model, log=\"all\")\n\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n    best_loss = float('inf')\n    early_stopping_counter = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_accuracy = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n\n        for images, input_ids, attention_mask, labels in progress_bar:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                outputs = model(images, input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            accuracy = calculate_accuracy(outputs, labels)\n            running_accuracy += accuracy\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = running_accuracy / len(train_loader)\n\n        train_losses.append(epoch_loss)\n        train_accuracies.append(epoch_accuracy)\n\n        wandb.log({\"train_loss\": epoch_loss, \"train_accuracy\": epoch_accuracy, \"epoch\": epoch+1})\n\n        model.eval()\n        val_running_loss = 0.0\n        val_running_accuracy = 0.0\n\n        with torch.no_grad():\n            val_progress_bar = tqdm(val_loader, desc=\"Validating\", unit=\"batch\")\n            for images, input_ids, attention_mask, labels in val_progress_bar:\n                images = images.to(device)\n                input_ids = input_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                labels = labels.to(device)\n\n                with torch.cuda.amp.autocast():\n                    outputs = model(images, input_ids, attention_mask)\n                    loss = criterion(outputs, labels)\n\n                val_running_loss += loss.item()\n                accuracy = calculate_accuracy(outputs, labels)\n                val_running_accuracy += accuracy\n\n        val_loss = val_running_loss / len(val_loader)\n        val_accuracy = val_running_accuracy / len(val_loader)\n\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n\n        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"epoch\": epoch+1})\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss}, Training Accuracy: {epoch_accuracy}\")\n        print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n\n        # Check if the validation loss improved\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_model_state = model.state_dict()\n            early_stopping_counter = 0  # Reset counter if we get a new best loss\n            print(f\"Saving model with lowest validation loss: {best_loss:.4f}\")\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scaler_state_dict': scaler.state_dict(),\n                'best_loss': best_loss,\n                'train_losses': train_losses,\n                'val_losses': val_losses,\n                'train_accuracies': train_accuracies,\n                'val_accuracies': val_accuracies\n            }, save_path)\n        else:\n            early_stopping_counter += 1\n            print(f\"No improvement in validation loss for {early_stopping_counter} epochs.\")\n\n        # Check for early stopping\n        if early_stopping_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    # Save the final metrics\n    metrics = {\n        \"train_losses\": train_losses,\n        \"val_losses\": val_losses,\n        \"train_accuracies\": train_accuracies,\n        \"val_accuracies\": val_accuracies\n    }\n\n    wandb.finish()\n    \n    return metrics\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_accuracy = 0.0\n\n    def calculate_accuracy(outputs, labels):\n        _, preds = torch.max(outputs, 1)\n        correct = (preds == labels).float().sum()\n        accuracy = correct / labels.size(0)\n        return accuracy.item()\n\n    with torch.no_grad():\n        progress_bar = tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\")\n        for images, input_ids, attention_mask, labels in progress_bar:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            with torch.cuda.amp.autocast():\n                outputs = model(images, input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n            accuracy = calculate_accuracy(outputs, labels)\n            running_accuracy += accuracy\n\n    loss = running_loss / len(data_loader)\n    accuracy = running_accuracy / len(data_loader)\n\n    print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n\n    return loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:56:45.664669Z","iopub.execute_input":"2024-12-06T15:56:45.665137Z","iopub.status.idle":"2024-12-06T15:56:45.680587Z","shell.execute_reply.started":"2024-12-06T15:56:45.665112Z","shell.execute_reply":"2024-12-06T15:56:45.679834Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import json\n\nmetrics = train_model(model, train_loader, val_loader, criterion, optimizer, scaler, device, num_epochs, patience, \"/kaggle/working/best_model.pth\", \"VQA_BASELINE(ResNet50-Bert)\")\nwith open(\"/kaggle/working/metrics.json\", \"w\") as f:\n    json.dump(metrics, f)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T15:57:16.979113Z","iopub.execute_input":"2024-12-06T15:57:16.979785Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:z1dss336) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">apricot-wind-3</strong> at: <a href='https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29/runs/z1dss336' target=\"_blank\">https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29/runs/z1dss336</a><br/> View project at: <a href='https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29' target=\"_blank\">https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241206_155026-z1dss336/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:z1dss336). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.19.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241206_155716-zl53tzyx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29/runs/zl53tzyx' target=\"_blank\">dainty-plant-4</a></strong> to <a href='https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29' target=\"_blank\">https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29/runs/zl53tzyx' target=\"_blank\">https://wandb.ai/muskanmotwani_24rco01-delhi-technological-university/VQA_BASELINE%28ResNet50-Bert%29/runs/zl53tzyx</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/100: 100%|██████████| 156/156 [03:00<00:00,  1.16s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100, Training Loss: 4.9186168511708575, Training Accuracy: 0.0853105709911921\nValidation Loss: 3.894217917552361, Validation Accuracy: 0.1962362001530635\nSaving model with lowest validation loss: 3.8942\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100: 100%|██████████| 156/156 [03:01<00:00,  1.16s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/100, Training Loss: 4.0538977323434295, Training Accuracy: 0.16583199789508796\nValidation Loss: 3.426269933199271, Validation Accuracy: 0.22587621670502883\nSaving model with lowest validation loss: 3.4263\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100: 100%|██████████| 156/156 [03:01<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.12s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/100, Training Loss: 3.690166189120366, Training Accuracy: 0.19864004630690965\nValidation Loss: 3.030592001401461, Validation Accuracy: 0.27243218781092227\nSaving model with lowest validation loss: 3.0306\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100: 100%|██████████| 156/156 [03:03<00:00,  1.18s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/100, Training Loss: 3.4121843729263697, Training Accuracy: 0.22630282529653648\nValidation Loss: 2.7697571974534254, Validation Accuracy: 0.3249458393607384\nSaving model with lowest validation loss: 2.7698\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100: 100%|██████████| 156/156 [03:01<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/100, Training Loss: 3.1857179754819627, Training Accuracy: 0.24784840929966706\nValidation Loss: 2.466833336231036, Validation Accuracy: 0.35424827879820114\nSaving model with lowest validation loss: 2.4668\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/100, Training Loss: 3.044602938187428, Training Accuracy: 0.267342562858875\nValidation Loss: 2.333891448302147, Validation Accuracy: 0.37851302822430927\nSaving model with lowest validation loss: 2.3339\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100: 100%|██████████| 156/156 [03:03<00:00,  1.18s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/100, Training Loss: 2.8677846988042197, Training Accuracy: 0.28325691475318027\nValidation Loss: 2.2026509520335074, Validation Accuracy: 0.392754332950482\nSaving model with lowest validation loss: 2.2027\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/100, Training Loss: 2.7836144062188954, Training Accuracy: 0.2871112298124876\nValidation Loss: 2.0997103796555447, Validation Accuracy: 0.4107943077882131\nSaving model with lowest validation loss: 2.0997\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/100, Training Loss: 2.673574462915078, Training Accuracy: 0.30102237657858777\nValidation Loss: 2.0257035356301527, Validation Accuracy: 0.44162511863769627\nSaving model with lowest validation loss: 2.0257\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:55<00:00,  1.12s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/100, Training Loss: 2.6357464347130213, Training Accuracy: 0.31089372627246076\nValidation Loss: 1.979168564845354, Validation Accuracy: 0.42656398392640626\nSaving model with lowest validation loss: 1.9792\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/100, Training Loss: 2.5432940553396177, Training Accuracy: 0.3300614316876118\nValidation Loss: 1.838409682114919, Validation Accuracy: 0.46291473775337905\nSaving model with lowest validation loss: 1.8384\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/100, Training Loss: 2.5085060680523896, Training Accuracy: 0.328677736222744\nValidation Loss: 1.8763201152667022, Validation Accuracy: 0.45969106142337507\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:55<00:00,  1.12s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/100, Training Loss: 2.4663032583701305, Training Accuracy: 0.33683523268271715\nValidation Loss: 1.6805950433779986, Validation Accuracy: 0.5054272019710296\nSaving model with lowest validation loss: 1.6806\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/100, Training Loss: 2.4263244767983756, Training Accuracy: 0.34370919393423277\nValidation Loss: 1.7365112740259905, Validation Accuracy: 0.4757723468236434\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/100, Training Loss: 2.3793153869800077, Training Accuracy: 0.35622180673556453\nValidation Loss: 1.6607557076674242, Validation Accuracy: 0.5089328109453886\nSaving model with lowest validation loss: 1.6608\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/100, Training Loss: 2.3444651395846634, Training Accuracy: 0.35435585830456173\nValidation Loss: 1.592415147102796, Validation Accuracy: 0.5263161800610714\nSaving model with lowest validation loss: 1.5924\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100: 100%|██████████| 156/156 [03:04<00:00,  1.18s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/100, Training Loss: 2.29237614151759, Training Accuracy: 0.36762152784145796\nValidation Loss: 1.5985524891278682, Validation Accuracy: 0.5148348468236434\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/100, Training Loss: 2.30149396107747, Training Accuracy: 0.3703444028894107\nValidation Loss: 1.508853404185711, Validation Accuracy: 0.5473053180254422\nSaving model with lowest validation loss: 1.5089\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/100, Training Loss: 2.265436128163949, Training Accuracy: 0.3852423137197128\nValidation Loss: 1.504692799005753, Validation Accuracy: 0.5447196998657324\nSaving model with lowest validation loss: 1.5047\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100: 100%|██████████| 156/156 [03:04<00:00,  1.18s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/100, Training Loss: 2.2438136606644363, Training Accuracy: 0.38435941953689623\nValidation Loss: 1.5145688779078996, Validation Accuracy: 0.5423232729618366\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/100, Training Loss: 2.2252867030791745, Training Accuracy: 0.3802676875239763\nValidation Loss: 1.4523618454352403, Validation Accuracy: 0.5546541133752236\nSaving model with lowest validation loss: 1.4524\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/100, Training Loss: 2.1951326391635795, Training Accuracy: 0.3921051460963029\nValidation Loss: 1.3869920189564044, Validation Accuracy: 0.567956879352912\nSaving model with lowest validation loss: 1.3870\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100: 100%|██████████| 156/156 [03:04<00:00,  1.18s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/100, Training Loss: 2.1697482810570645, Training Accuracy: 0.3955922067547456\nValidation Loss: 1.4940255819222865, Validation Accuracy: 0.5454467890354303\nNo improvement in validation loss for 1 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/100, Training Loss: 2.1646610101064048, Training Accuracy: 0.39952813394558734\nValidation Loss: 1.4380224263056731, Validation Accuracy: 0.5539604108303021\nNo improvement in validation loss for 2 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100: 100%|██████████| 156/156 [03:02<00:00,  1.17s/batch]\nValidating: 100%|██████████| 156/156 [02:53<00:00,  1.11s/batch]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/100, Training Loss: 2.14766075137334, Training Accuracy: 0.3965233262532797\nValidation Loss: 1.395607822598555, Validation Accuracy: 0.5529328408913735\nNo improvement in validation loss for 3 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100:   6%|▋         | 10/156 [00:13<02:52,  1.18s/batch]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Plot the result\n# Load the saved metrics\nwith open(\"/kaggle/working/metrics.json\", \"r\") as f:\n    metrics = json.load(f)\n\ntrain_losses = metrics[\"train_losses\"]\nval_losses = metrics[\"val_losses\"]\ntrain_accuracies = metrics[\"train_accuracies\"]\nval_accuracies = metrics[\"val_accuracies\"]\n\n# Plot the metrics\nepochs = range(1, len(train_losses) + 1)\n\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_accuracies, label='Training Accuracy')\nplt.plot(epochs, val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:20:03.875382Z","iopub.execute_input":"2024-07-04T05:20:03.875674Z","iopub.status.idle":"2024-07-04T05:20:04.51206Z","shell.execute_reply.started":"2024-07-04T05:20:03.875643Z","shell.execute_reply":"2024-07-04T05:20:04.51116Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VQAModel_trained(nn.Module):\n    def __init__(self, num_answers):\n        super(VQAModel_trained, self).__init__()\n        # Image feature extractor\n        self.cnn = models.resnet50(pretrained=True)\n        self.cnn.fc = nn.Identity()  # Remove the final classification layer\n\n        # Question feature extractor\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Fusion and final classification\n        self.fc1 = nn.Linear(2048 + 768, 1024)\n        self.fc2 = nn.Linear(1024, num_answers)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, images, input_ids, attention_mask):\n        # Extract image features\n        image_features = self.cnn(images)\n\n        # Extract question features\n        outputs = self.bert(input_ids, attention_mask)\n        question_features = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, 768)\n\n        # Concatenate features\n        combined_features = torch.cat((image_features, question_features), dim=1)\n\n        # Classification\n        x = self.fc1(combined_features)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:20:27.607139Z","iopub.execute_input":"2024-07-04T05:20:27.607544Z","iopub.status.idle":"2024-07-04T05:20:27.616385Z","shell.execute_reply.started":"2024-07-04T05:20:27.607511Z","shell.execute_reply":"2024-07-04T05:20:27.615306Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = VQAModel_trained(num_answers=582)  # Adjust num_answers based on your dataset\nmodel.to(device)\n# Load the best model checkpoint\ncheckpoint_path = \"/kaggle/working/best_model.pth\"\ncheckpoint = torch.load(checkpoint_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nprint(\"Best model loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:20:29.313064Z","iopub.execute_input":"2024-07-04T05:20:29.313832Z","iopub.status.idle":"2024-07-04T05:20:30.806053Z","shell.execute_reply.started":"2024-07-04T05:20:29.313796Z","shell.execute_reply":"2024-07-04T05:20:30.805112Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Example usage\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_dataset = VQADataset('/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/data_eval.csv', '/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/images', transform)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nprint(\"Test data loader prepared successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:20:33.429889Z","iopub.execute_input":"2024-07-04T05:20:33.430554Z","iopub.status.idle":"2024-07-04T05:20:34.290266Z","shell.execute_reply.started":"2024-07-04T05:20:33.430521Z","shell.execute_reply":"2024-07-04T05:20:34.289366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:38:59.364474Z","iopub.execute_input":"2024-07-04T05:38:59.364877Z","iopub.status.idle":"2024-07-04T05:44:56.483871Z","shell.execute_reply.started":"2024-07-04T05:38:59.364841Z","shell.execute_reply":"2024-07-04T05:44:56.482928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:51:31.695589Z","iopub.execute_input":"2024-07-04T05:51:31.695976Z","iopub.status.idle":"2024-07-04T05:51:44.208095Z","shell.execute_reply.started":"2024-07-04T05:51:31.695942Z","shell.execute_reply":"2024-07-04T05:51:44.206957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer, util\n\n# Load Sentence-BERT model\nmodel_sbert = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Function to calculate similarity using Sentence-BERT\ndef sbert_similarity(sentence1, sentence2):\n    embeddings1 = model_sbert.encode(sentence1, convert_to_tensor=True)\n    embeddings2 = model_sbert.encode(sentence2, convert_to_tensor=True)\n    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n    return cosine_scores.item()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:51:44.210285Z","iopub.execute_input":"2024-07-04T05:51:44.210689Z","iopub.status.idle":"2024-07-04T05:51:57.605968Z","shell.execute_reply.started":"2024-07-04T05:51:44.210655Z","shell.execute_reply":"2024-07-04T05:51:57.604536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_answer(answer):\n    # Remove underscores from the answer\n    return answer.replace('_', ' ')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:52:40.571908Z","iopub.execute_input":"2024-07-04T05:52:40.572642Z","iopub.status.idle":"2024-07-04T05:52:40.577232Z","shell.execute_reply.started":"2024-07-04T05:52:40.572607Z","shell.execute_reply":"2024-07-04T05:52:40.576254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load answer space for mapping\nwith open(os.path.join(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/\", \"answer_space.txt\")) as f:\n    answer_space = f.read().splitlines()\n\nmodel.eval()\ntest_losses = []\ntest_accuracies = []\nsimilarities = []\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:52:48.799071Z","iopub.execute_input":"2024-07-04T05:52:48.799695Z","iopub.status.idle":"2024-07-04T05:52:48.815956Z","shell.execute_reply.started":"2024-07-04T05:52:48.799663Z","shell.execute_reply":"2024-07-04T05:52:48.815231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    for idx, (images, input_ids, attention_mask, labels) in enumerate(test_loader):\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        outputs = model(images, input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n\n        test_losses.append(loss.item())\n\n        _, preds = torch.max(outputs, 1)\n        accuracy = calculate_accuracy(outputs, labels)\n        test_accuracies.append(accuracy)\n\n        predicted_answer = preprocess_answer(answer_space[preds.item()])\n        actual_answer = preprocess_answer(answer_space[labels.item()])\n\n        similarity = sbert_similarity(predicted_answer, actual_answer)\n        similarities.append(similarity)\n\n        # Plotting the image with question and answers\n        image = images.cpu().squeeze().permute(1, 2, 0).numpy()\n        image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        image = np.clip(image, 0, 1)\n\n        plt.imshow(image)\n        plt.title(f\"Question: {test_dataset.data.iloc[idx]['question']}\\nPredicted: {predicted_answer}\\nActual: {actual_answer}\\nSBERT Similarity: {similarity:.4f}\")\n        plt.axis('off')\n        plt.show()\n\n        if idx >= 49:  # Display only 50 samples\n            break\n\n# Print average similarity score\naverage_similarity = np.mean(similarities)\nprint(f\"Average SBERT Similarity: {average_similarity:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:52:58.433566Z","iopub.execute_input":"2024-07-04T05:52:58.434431Z","iopub.status.idle":"2024-07-04T05:53:18.470538Z","shell.execute_reply.started":"2024-07-04T05:52:58.434386Z","shell.execute_reply":"2024-07-04T05:53:18.469691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot test accuracy and test loss\nplt.figure(figsize=(14, 6))\n\n# Test Loss\nplt.subplot(1, 2, 1)\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Batch')\nplt.ylabel('Loss')\nplt.title('Test Loss')\nplt.legend()\n\n# Test Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.xlabel('Batch')\nplt.ylabel('Accuracy')\nplt.title('Test Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:54:07.546713Z","iopub.execute_input":"2024-07-04T05:54:07.547461Z","iopub.status.idle":"2024-07-04T05:54:07.992105Z","shell.execute_reply.started":"2024-07-04T05:54:07.547429Z","shell.execute_reply":"2024-07-04T05:54:07.990978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Similarity\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 1, 1)\nplt.plot(similarities, label='SBERT Similarity')\nplt.xlabel('Batch')\nplt.ylabel('Similarity')\nplt.title('SBERT Similarity')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:54:21.740288Z","iopub.execute_input":"2024-07-04T05:54:21.740688Z","iopub.status.idle":"2024-07-04T05:54:21.984297Z","shell.execute_reply.started":"2024-07-04T05:54:21.74066Z","shell.execute_reply":"2024-07-04T05:54:21.983418Z"},"trusted":true},"outputs":[],"execution_count":null}]}